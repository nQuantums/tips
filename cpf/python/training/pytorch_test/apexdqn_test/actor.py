#!/usr/bin/env python
import time
import datetime
import multiprocessing as mp
import random
import numpy as np
from collections import namedtuple
import cv2
import torch
import psycopg2

import db_initializer
import tables
from env import make_local_env
from transition import Transition
from transition import NStepTransition
import model


class ExperienceBuffer(object):

	def __init__(self, n):
		"""
		Implements a circular/ring buffer to store n-step transition data used by the actor
		:param n:
		"""
		self.local_1step_buffer = list() #  To store single step transitions to compose n-step transitions
		self.local_nstep_buffer = list() #  To store n-step transitions b4 they r batched, prioritized and sent to replay mem
		self.idx = -1
		self.capacity = n
		self.gamma = 0.99

	def construct_nstep_transition(self, S, max_q):
		if self.idx == -1: #  Episode ended at the very first step in this n-step transition
			return
		#  Put the n_step_transition into a local memory store
		l1b = self.local_1step_buffer[0]
		self.local_nstep_buffer.append(NStepTransition(l1b.S, l1b.A, l1b.R, l1b.Gamma, S, max_q))
		#  Free-up the buffer
		self.local_1step_buffer.clear()
		#  Reset the memory index
		self.idx = -1

	def add(self, data, max_q):
		"""
		Add transition data to the Experience Buffer and calls update_buffer

		Updates the accumulated per-step discount and the partial return for every item in the buffer. This should be
		called after every new transition is added to the buffer

		:param data: tuple containing a transition data of type Transition(s, a, r, gamma, q)
		:return: None
		"""
		if self.idx + 1 < self.capacity:
			g = self.gamma
			l1b = self.local_1step_buffer
			l1b.append(data)
			blen = len(l1b)

			for i in range(blen - 1):
				l1b_cur = l1b[i]
				R = l1b_cur.R
				Gamma = 1
				for k in range(i + 1, blen):
					Gamma *= g
					R += Gamma * l1b[k].R
				l1b[i] = Transition(l1b_cur.S, l1b_cur.A, R, Gamma, l1b_cur.q)

			self.idx += 1
		else: # single-step buffer has reached its capacity, n. Compute
			#  Construct the n-step transition
			self.construct_nstep_transition(data.S, max_q)

	def get(self, batch_size):
		assert batch_size <= len(self.local_nstep_buffer), "Requested n-step transitions batch size is more than available"
		batch_of_n_step_transitions = self.local_nstep_buffer[:batch_size]
		del self.local_nstep_buffer[:batch_size]
		return batch_of_n_step_transitions


class Actor:

	def __init__(self, params, param_set_id, actor_id, status_dict, shared_state, shared_replay_mem):
		env_conf = params['env']
		actor_params = params['actor']
		learner_params = params['learner']

		model_formula = f'model.{learner_params["model"]}(self.state_shape, self.action_dim)'

		self.conn = psycopg2.connect(params["db"]["connection_string"])
		self.conn.autocommit = True
		self.cur = self.conn.cursor()
		self.param_set_id = param_set_id

		self.actor_id = actor_id # Used to compose a unique key for the transitions generated by each actor
		self.state_shape = tuple(env_conf['state_shape'])
		self.action_dim = env_conf['action_dim']
		self.status_dict = status_dict
		self.frame_num = self.state_shape[0]
		self.frames = np.zeros((0,) + self.state_shape[1:], dtype=np.float32)
		self.params = actor_params
		self.shared_state = shared_state
		self.T = self.params["T"]
		self.Q = eval(model_formula)
		self.Q.load_state_dict(shared_state["Q_state_dict"])
		self.env = make_local_env(env_conf['name'])
		self.local_experience_buffer = ExperienceBuffer(self.params["num_steps"])
		self.global_replay_queue = shared_replay_mem
		eps = self.params['epsilon']
		N = self.params['num_actors']
		alpha = self.params['alpha']
		self.epsilon = eps**(1 + alpha * self.actor_id / (N - 1))
		self.gamma = self.params['gamma']
		self.num_buffered_steps = 0 # Used to compose a unique key for the transitions generated by each actor
		self.rgb2gray_k = np.array([[0.299 / 255, 0.587 / 255, 0.114 / 255]], dtype=np.float32).T
		# self.rgb2gray = lambda x: np.dot(x.astype(np.float32), self.rgb2gray_k) # RGB to Gray scale
		# self.torch_shape = lambda x: np.reshape(self.rgb2gray(x), (1, x.shape[1], x.shape[0])) # WxHxC to CxWxH
		# self.obs_preproc = lambda x: np.resize(self.torch_shape(x), state_shape)
		self.max_q = 0
		self.wait_shared_memory_clear = actor_params['wait_shared_memory_clear']

		self.last_Q_state_dict_id = 0

	def policy(self, q):
		self.max_q, action = torch.max(q, 0)
		action = action.item()
		if random.random() < self.epsilon:
			action = random.randrange(0, len(q))
		return action

	def obs_preproc(self, obs):
		obs = cv2.resize(obs, (self.state_shape[2], self.state_shape[1]))
		obs = np.dot(obs.astype(np.float32), self.rgb2gray_k)
		cv2.imshow(f'Actor{self.actor_id}', obs)
		# cv2.waitKey(1)
		obs = obs.reshape((1,) + obs.shape[:2])
		if self.frames.shape[0] < self.frame_num:
			self.frames = np.concatenate((self.frames, obs), axis=0)
		else:
			self.frames = np.concatenate((self.frames[1:], obs), axis=0)
		return self.frames

	def compute_priorities(self, n_step_transitions):
		# Convert tuple to numpy array
		r = np.array([tr.R for tr in n_step_transitions], dtype=np.float32)
		g = np.array([tr.Gamma for tr in n_step_transitions], dtype=np.float32)
		max_q = np.array([tr.max_q for tr in n_step_transitions], dtype=np.float32)

		#  Calculate the absolute n-step TD errors
		t = r + g * max_q
		td_error = t - max_q
		priorities = np.abs(td_error)

		return priorities

	def run(self):
		"""
		A method to gather experiences using the Actor's policy and the Actor's environment instance.
		- Periodically syncs the parameters of the Q network used by the Actor with the latest Q parameters made available by
			the Learner process.
		- Stores the single step transitions and the n-step transitions in a local experience buffer
		- Periodically flushes the n-step transition experiences to the global replay queue
		:param T: The total number of time steps to gather experience
		:return:
		"""
		n_step_transition_batch_size = self.params['n_step_transition_batch_size']
		status_dict = self.status_dict

		t = tables.ActorData()
		record_type = t.get_record_type()
		record_insert = t.get_insert()
		cur = self.cur
		param_set_id = self.param_set_id
		actor_id = self.actor_id
		now = datetime.datetime.now
		ep_len = 0
		ep_reward = 0.0
		priorities = None

		# 3. Get initial state from environment
		self.env.reset()
		for _ in range(self.frame_num):
			obs, _, _, _ = self.env.step(0)
			obs = self.obs_preproc(last_obs)

		for t in range(self.T):
			with torch.no_grad():
				q = self.Q(torch.tensor(last_obs.reshape((1,) + obs.shape))).squeeze()

			# 5. Select the action using the current policy
			action = self.policy(q)
			q = q.numpy()

			# 6. Apply action in the environment
			next_obs, reward, done, _ = self.env.step(action)
			# self.env.render()
			# 7. Add data to local buffer
			self.local_experience_buffer.add(Transition(obs, action, reward, self.gamma, q), self.max_q)
			obs = self.obs_preproc(next_obs)
			ep_reward += reward
			ep_len += 1
			# if t % 1000 == 0:
			# 	print("Actor#", self.actor_id, "t=", t, "action=", action, "reward:", reward, "1stp_buf_size:", len(self.local_experience_buffer.local_1step_buffer))

			if done: # Not mentioned in the paper's algorithm
				print(f'Actor#: {self.actor_id} t: {t} ep_len: {ep_len} ep_reward: {ep_reward}')

				# Truncate the n-step transition as the episode has ended; NOTE: Reward is set to 0
				self.local_experience_buffer.construct_nstep_transition(obs, self.max_q)
				# Reset the environment
				obs = self.obs_preproc(self.env.reset())

				# DBへデータ登録
				record_insert(
				    cur,
				    record_type(param_set_id, actor_id, now(), status_dict['train_num'], t, ep_len, ep_reward, qS_t.tolist(),
				                action, priorities.tolist()))

				ep_len = 0
				ep_reward = 0.0

			# 8. Periodically send data to replay
			if len(self.local_experience_buffer.local_nstep_buffer) >= n_step_transition_batch_size:
				# 9. Get batches of multi-step transitions
				n_step_experience_batch = self.local_experience_buffer.get(n_step_transition_batch_size)
				# 10.Calculate the priorities for experience
				priorities = self.compute_priorities(n_step_experience_batch)
				# 11. Send the experience to the global replay memory
				if self.wait_shared_memory_clear:
					while 64 < self.global_replay_queue.qsize():
						time.sleep(0.001)
				self.global_replay_queue.put([priorities, n_step_experience_batch])

			# Learner からの共有パラメータが更新されていたらロードする
			id = status_dict['Q_state_dict_id']
			if id != self.last_Q_state_dict_id:
				# 13. Obtain latest network parameters
				self.Q.load_state_dict(self.shared_state["Q_state_dict"])
				self.last_Q_state_dict_id = id

			if cv2.waitKey(1) == 27 or status_dict['request_quit']:
				status_dict['request_quit'] = True
				break

		print(f'actor{self.actor_id} end')


if __name__ == "__main__":
	""" 
	Simple standalone test routine for Actor class
	"""
	import json
	import learner

	with open('parameters.json', 'r') as f:
		params = json.load(f)

	param_set_id = db_initializer.initialize(params)

	mp_manager = mp.Manager()
	status_dict = mp_manager.dict()
	shared_state = mp_manager.dict()
	shared_mem = mp_manager.Queue()

	status_dict['quit'] = False
	status_dict['Q_state_dict_stored'] = False
	status_dict['request_quit'] = False

	l = learner.Learner(params, param_set_id, status_dict, shared_state, shared_mem)

	actor = Actor(params, param_set_id, 0, status_dict, shared_state, shared_mem)
	actor.run()
